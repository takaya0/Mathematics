\documentclass[11pt, a4paper, english, dvipdfmx]{jsarticle}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[psamsfonts]{amssymb}
\usepackage{color}
\usepackage{ascmac}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fancybox}
\usepackage{enumerate}
\usepackage{verbatim}
\usepackage{subfigure}
\usepackage{proof}
\usepackage{listings}
\usepackage{otf}

\theoremstyle{definition}

%
%%%%%%%%%%%%%%%%%%%%%%
%ここにないパッケージを入れる人は，必ずここに記載すること.
%
%%%%%%%%%%%%%%%%%%%%%%
%ここからはコード表です.
%

%英語で定義や定理を書きたい場合こっちのコードを使うこと.

\newtheorem{Axiom+}{Axiom}[section]
\newtheorem{Definition+}[Axiom+]{Definition}
\newtheorem{Theorem+}[Axiom+]{Theorem}
\newtheorem{Proposition+}[Axiom+]{Proposition}
\newtheorem{Lemma+}[Axiom+]{Lemma}
\newtheorem{Example+}[Axiom+]{Example}
\newtheorem{Corollary+}[Axiom+]{Corollary}
\newtheorem{Claim+}[Axiom+]{Claim}
\newtheorem{Property+}[Axiom+]{Property}
\newtheorem{Attention+}[Axiom+]{Attention}
\newtheorem{Question+}[Axiom+]{Question}
\newtheorem{Problem+}[Axiom+]{Problem}
\newtheorem{Consideration+}[Axiom+]{Consideration}
\newtheorem{Alert+}{Alert}

\def\hoge<#1>{\langle #1 \rangle}
%commmand

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Hil}{\mathcal{H}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\Loss}{L_{D}}
\newcommand{\MLsp}{(\X, \Y, D, \Hil, L_{D})}
\newcommand{\W}{{\cal W}}
\newcommand{\cS}{{\cal S}}
\newcommand{\Wpm}{W^{\pm}}
\newcommand{\Wp}{W^{+}}
\newcommand{\Wm}{W^{-}}
\newcommand{\p}{\partial}
\newcommand{\Dx}{D_{x}}
\newcommand{\Dxi}{D_{\xi}}
\newcommand{\lan}{\langle}
\newcommand{\ran}{\rangle}
\newcommand{\pal}{\parallel}
\newcommand{\dip}{\displaystyle}
\newcommand{\e}{\varepsilon}
\newcommand{\dl}{\delta}
\newcommand{\pphi}{\varphi}
\newcommand{\ti}{\tilde}
\title{Extending some linear methods to nonlinear methods by positive definite kernel and RKHS}
\author{yataka}
\date{}
\begin{document}
\maketitle
\setcounter{section}{-1}
\section{Motivation and Notation}
\subsection{Motivation}
Thre are a lot of Machine Learning methods which approximate linear data.
However, most of data in this World are non-linear. Thus, we show you a method, called kernel method, that
embed data of input space into high dimentional vector space with inner.
\subsection{Notation}
\noindent
$\X$: input space\\
$\Y$: output space\\
$\L$: Loss function\\
$\K$: field\\
$V$: vector space\\
$(V, \|\cdot\|)$: normed space\\
$(V, \hoge<\cdot, \cdot>)$ : inner space\\
$\Hil$: hypothesis space or Hilbert space\\
$\Hil_{k}$: RKHS with reproducing kernel $k$\\
\section{Functional Analysis}
\begin{Definition+}(vector space)\\
    Let $\K$, $V$ be a field and a set with two operations, addition and scalar multiplication.
    If $V$ statisfies
    \begin{enumerate}
        \item $V$ becomes commutative group by addition,
        \item $\forall\alpha\in\K, \forall u, v\in V, \alpha(u + v) = \alpha u + \alpha v$,
        \item $\forall \alpha, \beta\in\K, \forall v\in V, (\alpha + \beta)v = \alpha v + \beta v$,
        \item $\forall \alpha, \beta\in\K, \forall v\in V, \alpha(\beta v) = (\alpha\beta)v$ and 
        \item $\exists 1_{\K}\in\K~\text{ s.t. }\forall v\in V, 1_{\K}v = v$.
    \end{enumerate}
    Then, $V$ is called $\mathbf{vector~space}$ over $\K$.
\end{Definition+}
We consider only real verctor space or complex vector space in this article. Thus, 
$\K = \R$ or $\K = \C$.
\begin{Definition+}(normed vector space)\\
    Let $V$, $\|\cdot\|$ be a vector space over $\K$ and a map from $V$ to $\K$.
    $\|\cdot\|$ is called $\mathbf{norm}$ on $\K$ when $\|\|$ statisfies 
    \begin{enumerate}
        \item $\forall v\in V, \|v\| \geq 0$,
        \item $\forall v\in V, \|v\| = 0 \iff v = 0$,
        \item $\forall \alpha\in\K, \forall v\in V, \|\alpha v\| = |\alpha|\|v\|$ and 
        \item $\forall v, w\in V, \|v + w\|\leq \|v\| + \|w\|$.
    \end{enumerate}
    A pair $(V, \|\|)$ is called $\mathbf{normed~vector~space}$ or $\mathbf{normed~space}$
\end{Definition+}
\begin{Proposition+}
    Suppose that $(V, \|\|)$ is normed space.
    Then, norm space is metric space by a distance $d:V\times V\to\K$ generated  by norm,
    \begin{align*}
        d(x, y) = \|x - y\|.
    \end{align*}
\end{Proposition+}

\begin{Definition+}(complete)\\
    Let $(X, d)$ be metric space.
    $(X, d)$ is called $\mathbf{complete}$ when every cauchy sequence in $X$ converges a element in $X$.
\end{Definition+}

\begin{Definition+}(compact)\\
    Let $(X, \mathscr{O})$ be topological space. 
    X is called compact if every open covering of $X$ has finite open covering. i.e.
    \begin{align*}
        \forall \{O_{\lambda}\}_{\lambda\in\Lambda}\subset\mathscr{O},  X = \bigcup_{\lambda\in\Lambda}O_{\lambda}\Longrightarrow\exists\lambda_{1}, \lambda_{n}, \cdots, \lambda_{n}\text{ s.t. }X = \bigcup_{i = 1}^{n}O_{\lambda_{i}}
    \end{align*}
\end{Definition+}

\begin{Definition+}(Banach space)\\
    Let $(V, \|\|)$ be normed space. 
    $(V, \|\|)$ is called $\mathbf{Banach~space}$ when $V$ is complete by a distance generated by norm.
\end{Definition+}
Banach space is very important space in mathematics. 
Therefore, I show you some examples of Banach space.
\begin{Example+}($C[X]$)\\
    Let $X$ be compact space. Suppose that $C[X]$ is a collection of all continuous functions on $X$.
    We define a norm on $X$ below:
    \begin{align*}
        \|x\|_{\infty} = \max\{|x(t)|~|t\in X\}.
    \end{align*}
    Then, $(C[X],\|\|_{\infty})$ becomes Banach space.
\end{Example+}
\begin{Example+}($\mathcal{L}^p(a, b)$, $p\geq 1$)\\
    Let $(a, b)$ be interval. Suppose that $((a, b), \mathcal{B}(a, b), L)$ is 
    measure space where $\mathcal{B}(a, b)$ is Borel space and $L$ is Lebesugue measure. we define $L^{p}(a, b)$ as
    \begin{align*}
        L^p(a, b) := \left\{x:(a, b)\to\K~|\int_{a}^{b}|x(t)|^{p}dt < \infty\right\}
    \end{align*} 
    and a norm as
    \begin{align*}
        \|x\|_{p} = \left(\int_{a}^{b}|x(t)|^{p}dt\right)^{\frac{1}{p}}.
    \end{align*}
    We consider a equivarence relation $\sim$ on $L^{p}(a, b)$. $x\sim y$ is defined 
    $x(t) = y(t)~a.s.t\in (a, b)$. Then, a collection of all equivarence classes on $L^{p}(a, b)$ is donated 
    $\mathcal{L}^{p}(a, b)$ and becomes a vector space under addition and scalar multiplication defined by
    \begin{align*}
        (x + y)(y) = x(t) + y(t)\hspace{20pt}(\alpha x)(t) = \alpha x(t).
    \end{align*}

    A pair $(\mathcal{L}^{p}, \|\|_{p})$ is Banach space and called $\mathbf{L^{p}~space}$.
\end{Example+}
\begin{Definition+}(inner space)\\
    Let $V$ be vector space over $\K$. Suppose that $\hoge<\cdot, \cdot>$ is a map from $V\times V$ to $\K$
    which statisfies
    \begin{enumerate}
        \item $\forall v\in V, \hoge<v, v>\geq 0$,
        \item $\forall v\in V, \hoge<v, v> = 0\iff v = 0$,
        \item $\forall v, w\in V, \hoge<v, w> = \overline{\hoge<w, v>}$,
        \item $\forall u, v, w\in V, \hoge<u + v, w> = \hoge<u, w> + \hoge<v, w>$ and
        \item $\forall v, w\in V, \forall\alpha\in\K, \hoge<\alpha v, w> = \alpha\hoge<v, w>$.
    \end{enumerate}
    Then, a pair $(V, \hoge<\cdot, \cdot>)$ is called $\mathbf{inner~space}$ and $\hoge<\cdot, \cdot>$ is called $\mathbf{inner}$ on $V$.
\end{Definition+}

\begin{Proposition+}
    Let $(V, \hoge<\cdot, \cdot>)$ be inner space over $\K$.
    we difine a map $\|\cdot\|$ from $V$ to $\K$ as
    \begin{align*}
        \|x\| = \sqrt{\hoge<x, x>}.
    \end{align*}
    Then, $\|\cdot\|$ is a norm on $V$ and is called norm generated by inner. 
\end{Proposition+}
\begin{Definition+}(Hilbert space)\\
    Let $(V, \hoge<\cdot, \cdot>)$ be inner space.
    $V$ is called $\mathbf{Hilbert~space}$ if $V$ is banach space by a norm generated by inner.
\end{Definition+}

\section{Positive definate kernel}

\begin{Definition+}(positive definate kernel)\\
    Let $\X$, $k$ be a set and a map from $\X\times\X$ to $\K$.
    $k$ is called $\mathbf{positive~definate~kernel}$ on $\K$ if 
    $k$ has two conditions below:
    \begin{enumerate}
        \item $\forall x, y\in\X, k(x, y) = k(y, x)$ and
        \item $\forall n\in\N, \forall x_{1}, \cdots, x_{n}\in\X, \forall c_{1}, \cdots, c_{n}\in\R$,
              \begin{align*}
                  \sum_{i = 1}^{n}\sum_{j = 1}^{n}c_{i}c_{j}k(x_{i}, x_{j})\geq 0.
              \end{align*}
    \end{enumerate}
    second condition is called $\mathbf{definiteness}$. definiteness under symmetrically means that a matrix
    \begin{align*}
        \left(\begin{array}{ccc}
            {k\left(x_{1}, x_{1}\right)} & {\dots} & {k\left(x_{1}, x_{n}\right)} \\
            {\vdots} & {\ddots} & {\vdots} \\
            {k\left(x_{n}, x_{1}\right)} & {\cdots} & {k\left(x_{n}, x_{n}\right)}
        \end{array}\right)
    \end{align*}
    is positive-semidefinite. This symmetric matrix is called $\mathbf{gram~matrix}$.
\end{Definition+}
\begin{Proposition+}
    Let $k:\X\times\X\to\K$ be positive definate kernel. Then, 
    \begin{enumerate}
        \item $\forall x\in\X, k(x, x)\geq 0$,
        \item $\forall x, y\in\X, |k(x, y)|^2\geq k(x, x)k(y, y)$ and
        \item every subset $\Y$, $k_{|\Y\times\Y}$ is positive difine kernel too.
    \end{enumerate}
    \begin{proof}
        1.)~From definateness of $k$, $1\times 1\times k(x, x)\geq 0$ for every $x$ in $\X$. Hence, 
        $k(x, x)\geq 0$.\\
        2.) I can't prove....\\
        3.)~It is clear.
    \end{proof}
\end{Proposition+}

\begin{Proposition+}
    Let $\X$, $\{k_{n}\}_{n\in\N}$ be a set and a sequence of positive definate kernal on $\K$.
    Then, $\alpha k_{1} + \beta k_{2}(\alpha, \beta \geq 0)$, $k_{1}k_{2}$, $\dip \lim_{n\to\infty}k_{n}$ are also positive definate kernels.
    However, we asusme that 
    \begin{enumerate}
        \item $\forall x, y\in\X, k_{1}k_{2}(x, y) = k_{1}(x, y)k_{2}(x, y)$ and
        \item $\{k_{n}\}_{n\in\N}$ converges a map $k:\X\times\X\to\K$ .
    \end{enumerate}
    \begin{proof}
        just a moment please....
    \end{proof}
\end{Proposition+}

\begin{Proposition+}
    Let $\X$ be a set.
    \begin{enumerate}
        \item A non negative constant map $k:\X\times\X\to\{c\}~(c\in\R_{\geq 0})$ is positive definate kernel.
        \item Let $k:\X\times\X\to\C$ be positive definate kernal. Then, a map $k^{'}:\X\times\X\to\C$, defined by 
        \begin{align*}
            k^{'}(x, y) = f(x)k(x, y)\overline{f(y)},
        \end{align*} 
        is positive definate kernel for every function $f:\X\to\C$.
    \end{enumerate}
\end{Proposition+}

\begin{Proposition+}(kernel tric)\\
    Let $\X$, $(V, \hoge<\cdot, \cdot>)$ be a set and inner space. a map $k:\X\times\X\to\K$, defined by 
    \begin{align*}
        k(x, y) = \hoge<\Phi(x), \Phi(y)>,
    \end{align*}
    is positive definate kernal for every function $\Phi:\X\to V$.
\end{Proposition+}
\section{Reproducing Kernel Hilbert Space}
\begin{Definition+}(reproducing kernel Hilbert space)\\
    Let $\X$, $\Hil$ be a set and Hilbert space from some functions on $\X$.\\
    $\Hil$ is called $\mathbf{Reproducing~Kernel~Hilbert~Space}$ or $\mathbf{RKHS}$ simply when
    \begin{align*}
        \forall x\in\X, \exists k_{x}\in\Hil\text{ s.t. }\forall f\in\Hil, \hoge<f, k_{x}> = f(x).
    \end{align*} 
    A map $k:\X\times\X\to\K$, defined by $k(y, x) = k_{x}(y)$, is called $\mathbf{reproducing~kernel}$ of $\Hil$. 
\end{Definition+}

\begin{Proposition+}
    Let $\Hil_{k}$ be a RKHS. Then, the reproducing kernal of $\Hil_{k}$ is positive 
    definate kernal and is unique. 
    \begin{proof}
        For every $x_{1}, x_{2}, \cdots, x_{n}\in\X$ and $c_{1}, c_{2}, \cdots, c_{n}\in\K$, 
        \begin{eqnarray*}
            \sum_{i = 1, j = 1}^{n}c_{i}\overline{c_{j}}k(x_{i}, x_{j}) &=& 1\\
                                                                        &=& 2
        \end{eqnarray*}
    \end{proof}
\end{Proposition+}

\section{The Principle of Machine Learning}
Learning Theory is devided into three parts: Supervised Learning, Unsupervised Learning and Reinforcement Learning. 
This article explains only Supervised Learning.
Machine Learning and Deep Learning are Supervised Learning. The purpose of Supervised
Learning is learning ''good” functions from training data. In the next subsection, I explain basic definitions of
Firstly, we define a fundamental space of Machine Learning based on Statistical Learning theory.
\subsection{Fundametal space }
\begin{Definition+}(input space)\\
    $\mathcal{X}$ $:=$ $\R^{d}$ is called $\mathbf{input~space}$.
\end{Definition+}
\begin{Definition+}(output space)\\
    $\mathcal{Y}$ $:=$ $\R^{m}$ is called $\mathbf{output~space}$.
\end{Definition+}
\begin{Definition+}(Hypothesis space)\\
      Let $\X$, $\Y$ be a input space and a output space respectively. $\mathbf{Hypothesis~space}$, donated $\Hil$, is a set of function from $\X$ to $\Y$ with some restrictions. i.e.
    \begin{align*}
        \Hil = \{f:\X\to\Y~|~f\text{ stisfy some conditions}\}
    \end{align*}
      A element $f$ of $\Hil$ is called $\mathbf{hypothesis}$. The detail of conditions is explained later.
\end{Definition+}

\begin{Definition+}(Data)\\
    Let $\X$, $\Y$ be a input space and a output space respectively.
    A finite subset of $\X\times\Y$ is called Data sequence or Data simply. Data is donated $D$.
\end{Definition+}

\begin{Definition+}(Training data and Testing data)\\
    Data $D$ is devided into two disjoint data, Training data $S$ and Testing data $T$.
\end{Definition+}
\begin{Definition+}(Loss function)\\
    Let $\Hil$ be a Hypothesis space and $D$ be data. $\mathbf{Loss~function}$, donated $L_{D}$, is function from $\Hil$ to $\R$.
\end{Definition+}

\begin{Definition+}(Machine Learning space)\\
    Let $\X$, $\Y$, $D$, $\Hil$, $L_{D}:\Hil\to\R$ be a input space, output space, Data, Hypothesis space and Loss function respectively.
    Then, the -5 tuple $\MLsp$ is called $\mathbf{Machine~Learning}$ $\mathbf{space}$.
\end{Definition+}
\begin{Definition+}(Learning)\\
   Let $\Hil$ be a Hypothesis space and $L_{D}:\Hil\to\R$ be a loss function. A process that find a hypothesis $f_{opt}\in\Hil$ that minimalize $\{L_{D}(f)~|~f\in\Hil\}$ ~from Train data $S$ is called Learning.
   then, $f_{opt}\in\Hil$ is called optimal hypothesis.
\end{Definition+}

\begin{Attention+}
    Generally, $f_{opt}$ is not equal to $f_{min}$. ($f_{min}$ is argment of minimam $\{L_{D}(f)~|~f\in\Hil\}$)
\end{Attention+}

\begin{Definition+}(Machine Learning)\\
    Let $\MLsp$ be a Machine Learning space. A learning on $\MLsp$ is called Machine Learning.
\end{Definition+}
\newpage
\subsection{Some examples}
Simple Regression Analysis is the most fundametal Machine Learning Model.
\subsubsection{Setting Problem}
I define Machine Learning space as below:
$\X = \R$, $\Y = \R$,
\begin{align*}
    \Hil = \{f:\X\to\Y~|~f(x) = wx,~ w\in\R\},
\end{align*}
\begin{align*}
    L_{D}(f) = \sum_{i = 1}^{N} |f(x_{i}) - t_{i}|^{2}~~~~(f\in\Hil)
\end{align*}
Then, Learning in this space is called Simple Regression Analysis.

\subsubsection{Solution of this Problem}
Let $f\in\Hil$ be $f(x) = wx$. then,
\begin{eqnarray*}
  \frac{dL}{dw} &=& \frac{d}{dw}\sum_{i = 1}^{N}(wx_{i} - t_{i})^{2}\\
                &=& \sum_{i = 1}^{N}2x_{i}(wx_{i} - t_{i})\\
                &=& 2\sum_{i = 1}^{N}x_{i}(wx_{i} - t_{i}) = 0
\end{eqnarray*}
So optimal hypothesis in this problem is
\begin{align*}
    f_{opt}(x) = \frac{\sum_{i = 1}^{N}x_{i}t_{i}}{\sum_{i = 1}^{N} x_{i}^{2}} x.
\end{align*}

\subsubsection{Improving this model}
The model above has many improving points. For examples
\begin{enumerate}
    \item It is difficult to predict non linear data.
    \item In many case, there are more than one factor.
\end{enumerate}
Firstly, I think about second problem. I think of some independent reasons  as one vector.
\subsection{Multiple Regression Analysis}
\subsubsection{Setting problem}
I define Machine Learning space as below: $\X = \R^{d}$, $\Y = \R$,
\begin{align*}
    \Hil = \{f:\X\to\Y~|~f(x) = Wx,~ W\in\R^{1\times d}\},
\end{align*}
\begin{align*}
    L_{D}(f) = \sum_{i = 1}^{N} |f(x_{i}) - t_{i}|^{2}~~~~(f\in\Hil)
\end{align*}
Then, Learning in this space is called Multiple Regression Analysis.

\subsubsection{Solution of this Problem}
Let $X$, $w$, $t$ be $N\times d$ Input Data matrix that $i$ th row is $i$th Input data, parametor vector in $\R^{1\times d}$ and target vector in $\R^{N}$ respectively. i.e.
\begin{align*}
    X =
    \left[ \begin{array}{cccc} {x_{11}} & {x_{12}} & {\dots} & {x_{1 d}} \\
   {x_{21}} & {x_{22}} & {\dots} & {x_{2 d}} \\
   {\vdots} & {\vdots} & {\ddots} &  {\vdots}\\
  {x_{N 1}} & {x_{N 2}} & {\dots} & {x_{N d}}\end{array}\right],
\end{align*}
$t = (t_{1}, t_{2}, \cdots, t_{N})^{T}$ and $w = (w_{1}, w_{2}, \cdots, w_{d})^{T}$~ ($T$ refer Transpose of vector or matrix).\\
Optimal hypothesis in this problem is
\begin{align*}
    f_{opt}(x) = ((X^{T}X)^{-1}X^{T}t)^{T}x.
\end{align*}

\section{Kernel method}
I showed some methods in section 4. However, these methods is for linear data. 
If you have learned Machine Learning, You may know Neural Network for non linear data.
However, I explain Neural Network after this section and show new method, Kernel method, for non linear data in this section. 

\section{Neural Network}
\begin{Definition+}(Shallow Neural Network for Regression)\\
    we difine Machine Learning space as below: $\X = \R^{d}$, $\Y = \R$,
    \begin{align*}
        \Hil_{SNN} = \{f:\X\to\Y~|~ f(x) = W_{2} \eta\left(W_{1} x-b_{1}\right) - b_{2},~W_{1} \in \R^{L \times d}, W_{2} \in \R^{1 \times L}, b_{1} \in \R^{L}, b_{2}\in\R\},
    \end{align*}
    \begin{align*}
        L(f) =  \frac{1}{N}\sum_{i = 1}^{N} |f(x_{i}) - t_{i}|^{2}
    \end{align*}
    ,where $\eta:\R^{L}\to\R^{L}$ is activation function, non linear and  Lipschitz continuous.
    A element of $\Hil_{SNN}$ is called $\mathbf{Shallow~Neural~Network~for~Regression}$.
\end{Definition+}
\begin{thebibliography}{9}
    \bibitem{neural} https://tutorials.chainer.org/ja/index.html
    \bibitem{ML} https://github.com/Runnrairu/machinelearning\verb|_|text
    \bibitem{kernel} カーネル法入門―正定値カーネルによるデータ解析・福水健次・2010
    \bibitem{ML1} 統計的学習理論・金森 敬文・2015
    \bibitem{functional} 函数解析 POD版・前田 周一郎・2007
\end{thebibliography}
\end{document}
